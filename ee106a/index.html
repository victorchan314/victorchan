<!DOCTYPE html>
<html lang="en">

    <head>

        <title>EE 106A Final Project</title>

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Latest compiled and minified CSS -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

        <!-- Optional theme -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">

        <!-- Latest compiled and minified JavaScript -->
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

        <link rel="stylesheet" type="text/css" href="styles.css">

    </head>


    <body>

        <div class="container-fluid">

            <div class="main row">

                <img class="img-responsive" src="img/jerome1_cropped.jpg"></img>

                <center><h1>Acroquad</h1></center>

                <nav class="navbar navbar-default">
                    <ul class="nav navbar-nav">
                        <li class="active"><a href="#">Home</a></li>
                        <li><a href="#introduction">Introduction</a></li>
                        <li><a href="#team">Team</a></li>
                        <li><a href="#design">Design</a></li>
                        <li><a href="#hardware">Hardware</a></li>
                        <li><a href="#software">Software</a></li>
                        <li><a href="#system">System</a></li>
                        <li><a href="#challenges">Challenges</a></li>
                        <li><a href="#results">Results</a></li>
                        <li><a href="#conclusion">Conclusion</a></li>
                        <li><a href="#additional_information">Additional Information</a></li>
                    </ul>
                </nav>

                <div id="introduction" class="col-md-12">
                    <center><h1>Introduction</h1></center>

                    <p>
Our Goal: To assemble a UAV with an onboard computer that responds to visual commands by actuating in a corresponding movement.
                    </p>
                    <p>
Unmanned aerial vehicles are currently a popular branch of robotics, with leading researchers integrating computer vision and machine learning to reduce human error in controlling drones. Because both fields are relatively new, current researchers are still developing better and more sophisticated ways to control quadcopters with machine learning! Our group was interested in combining racing drones and computer vision to create a quadcopter that responds to camera inputs. The project was challenging, as we had to not only construct a quadcopter with an onboard computer, but also run an intensive computer vision algorithm on the quadcopter and figure out how to control the drone with the outputs from the algorithm. The applications of such a product are limitless, including autonomous disaster search and rescue, subject tracking in cinematography, and automatic obstacle avoidance for UAVs.
                    </p>

                </div>

                <div id="team" class="col-md-12">
                    <center><h1>Team</h1></center>

                    <table id="team_information" class="table table-responsive">
                        <tr>
                            <td>Name</td>
                            <td>Contact</td>
                            <td>Bio</td>
                            <td>Contribution</td>
                        </tr>
                        <tr>
                            <td>Chong Zhao</td>
                            <td>chongzhao97@gmail.com</td>
                            <td>
I am an exchange student in EE department from China mainland and my major in home university is Physics. I am familiar with mechanics and mathematics skills such as random processes etc.
                            </td>
                            <td>
I worked with Hongjian on the computer vision tracking algorithm in Open CV. I have tried many vision programs including edge detecting, convolution algorithms etc. Besides that, I improved the robust of the final algorithms. I also helped debug issues that occurred when transferring the program to the raspberry pi and helped Hongjian Duan to deal with the system and communication problem. Thank you! 
                            </td>
                        </tr>
                        <tr>
                            <td>Hongjian Duan</td>
                            <td>duanhongjian15@mails.ucas.ac.cn</td>
                            <td>
I am a 4th year exchange EE student, and major in CS in my home school. I have learned all the main courses in CS and have a lot experience in database and operating system and Computer Architecture and has learned the intro to AI. This project gave me a better understanding of the hardware and the control system (Another course EE221A Linear system theory in this semester). 
                            </td>
                            <td>
I worked with Chong Zhao on the computer vision algorithm to track the tennis ball and Identify its direction of motion. We successfully designed a program to perform tracking and identification on the Pi on which the computational resources are very limited. I also debugged communication issues between the drone and the raspberry pi and helped set parameters in rosflight for successful communication.
                            </td>
                        </tr>
                        <tr>
                            <td>Isabella Maceda</td>
                            <td>imaceda@berkeley.edu</td>
                            <td>
I am a undergraduate Electrical Engineering and Computer Science major at UC Berkeley. I have a background working with drones and I am co-president of UAVs@Berkeley, an on-campus drone student organization. I was inspired to do the AcroQuad project by a friend and member of UAVs@Berkeley. This project was the perfect opportunity for me to combine two passions of mine, drones and machine learning/computer vision. Aside from drones, my hobbies include baking and learning new languages.
                            </td>
                            <td>
For this project, I was the leader for all hardware aspects. I assembled the drone and debugged all hardware malfunctions. I also was in charge of testing out the hardware components and uploading different firmwares on the flight controller. I also played a role in setting up the parameters in the rosflight program so the software could communicate with the hardware. Lastly, I helped to integrate the computer vision aspect with the physical drone, along with my other team members.
                            </td>
                        </tr>
                        <tr>
                            <td>Victor Chan</td>
                            <td>victorchan@berkeley.edu</td>
                            <td>
Hi! I am an EECS undergraduate student at UC Berkeley. I have a background in machine learning and robotics, so for me, Acroquad was the perfect combination of esoteric hardware debugging and annoying coding problems. In my spare time I teach the Parkour Decal at Berkeley (check us out) and go bouldering at Bridges!
                            </td>
                            <td>
I worked on everything related to the Raspberry Pi and ROS, from setting up Ubuntu with ROS on the Pi to writing the ROS node to translate between the computer vision outputs and ROSflight commands. I also worked on getting the camera feed running and configuring headless setup of the Pi.
                            </td>
                        </tr>
                    </table>

                </div>

                <div id="design" class="col-md-12">
                    <center><h1>Design</h1></center>

                    <p>
For our design criteria, we decided that our project must have a working, flying quadcopter. It should process visual input from a camera and use that input to control the quadcopter. Because drones are notorious for being inconsistent, we do not have any lower limit for reliability, except that it should work at least some of the time; however, the separate components should function properly.
                    </p>
                    <p>
Our final design pits a Raspberry Pi equipped with Ubuntu 16.04 and the ROS Kinetic as the onboard computer. It is connected to the flight controller of the drone and uses ROSflight to communicate with the drone, a small generic FPV racing quadcopter. The software is all written in Python, which in our opinion, is the most suitable language for both ROS and for computer vision. It integrates the OpenCV computer vision function and the ROSflight firmware with a ROS node as the link between them. As a result, the sensing and actuation scripts are handled together in the ROS node. In our finalized version, our computer vision script tracks a bright yellow circle (e.g. a tennis ball) and sends movement commands to the motors based on the direction the ball is moved.
                    </p>
                    <p>
Using the Raspberry Pi was a trade-off between expensive computing power and simple adaptability. We weren’t sure exactly how the code would be run, and with our budget of $50, we decided that the Raspberry Pi, while not optimal, was good enough to meet our design criteria. In a real-world situation, the expensive Nvidia Jetson would be more likely to meet any reliability criteria. Another related trade-off was between running the computer vision on the onboard or offboard computer. The offboard computer was arguably faster and better-equipped to handle the code, but getting the video feed to the camera over the network would be far too slow. Because the Raspberry Pi was enough to handle the terrible resolution of the Logitech webcams, we were able to bypass the offboard computer. We were limited to tracking “easier” objects, such as a bright yellow circle instead of human body parts. Again, in an industry setting, a more powerful onboard computer would be a great starting solution to allowing more computer power.
                    </p>

                </div>

                <div id="implementation" class="col-md-12">

                    <div id="hardware" class="col-md-12">
                        <center><h1>Hardware</h1></center>

                        <p>
The drone has a Naze 32 F1 flight controller because of its compatibility with the ROSflight firmware. Since we used an F1 controller we had to use a FrSky XRS receiver that had a PPM signal, as specified in the ROSflight documentation. We decided on a Raspberry Pi 3B+ with a 32GB SD card as our onboard computer, with a Logitech webcam for the image processing. Lastly, we used a Taranis QX7 transmitter because it is compatible with our receiver and has an easy-to-use user interface for setting up flight models.
                        </p>
                        <p>
Our quadcopter frame was a generic racing FPV drone frame. It took care of most of the parts, but because we used a webcam instead of an FPV camera, we had to create a custom 3D-print piece to attach the webcam onto the drone. Because it was difficult to design the perfect custom part, we also included a little bit of duct tape to secure the webcam position. The webcam is connected to the Raspberry Pi at a USB port. Finally, we firmly and cleanly fastened the Raspberry Pi to the quadcopter with some double-sided tape and connected it to the F1 controller with a USB-to-microUSB cable.
                        </p>

                    </div>

                    <div id="software" class="col-md-12">
                        <center><h1>Software</h1></center>

                        <p>
Our software consists of two main parts: the ROS nodes, and the computer vision algorithm. Everything is written in Python2.7 and runs on the Raspberry Pi, to which we downloaded Ubuntu 16.04 and installed ROS kinetic. We actually discovered that Ubiquity Robotics provides a Ubuntu 16.04 image with ROS kinetic already installed, which made the flashing process a lot easier.
                        </p>
                        <p>
We have one main ROS node that acts as the connection between the computer vision and the quadcopter. Our ROS package, sight_to_flight, has one ROS node: commander. The ROS node calls the get_trace_of_ball function in the trace_ball package and receives 1 of 4 directions: 0 (up), 1 (down), 2 (left), and 3 (right). commander then generates a new Command message, defined in the ROSflight package, sets the parameters to correspond to the appropriate direction, and publishes it to the /command topic. It then restarts the get_trace_of_ball function and loops until the ROS program is quit.
                        </p>
                        <p>
Our computer vision code is all contained within the trace_ball package. The relevant code is contained within get_trace_of_ball_function.py, which uses OpenCV, NumPy, and Deque. The program begins by setting up parameters such as the max time to run, creates a deque, and opens up the webcam feed. It then runs a loop until it either is killed by commander or times out after about 30 seconds. Within the loop, it keeps track of each frame, at a rate of about 30 FPS, and detects a yellow circle (our image of a tennis ball) with OpenCV’s edge and color detection algorithms. The program compares the current center of the ball to the previously calculated center and determines the most probable direction that the ball has been travelling in (basically just the direction with the largest change) before adding that direction to the deque. After the deque is full, the program determines which direction the tennis ball has travelled in the most, and returns that as the output. If no direction is returned, the program returns -1.
                        </p>

                    </div>

                    <div id="system" class="col-md-12">
                        <center><h1>System</h1></center>

                        <p>
Our system is simply composed of the three separate modules: the quadcopter and its firmware and flight controller, the Raspberry Pi and ROS, and the computer vision and webcam. As described before, the F1 flight controller is connected to the Raspberry Pi with a USB-to-microUSB cable and controlled by ROSflight, and the computer vision outputs are converted to ROSflight commands through the custom ROS node commander. The system runs as follows:
                        </p>

                        <ol>
                            <li>
The quadcopter is turned on, powering the Raspberry Pi as well.
                            </li>
                            <li>
With an offboard computer, we connect to the Pi, which is set up for headless
                            </li>
                            <li>
We run the ROS node commander from the offboard computer
                            </li>
                            <li>
commander calls the computer vision algorithm, get_trace_of_ball
                            </li>
                            <li>
get_trace_of_ball takes in feed from the webcam and returns a direction (or no direction)
                            </li>
                            <li>
If no direction is detected, commander sleeps for 5 seconds and calls get_trace_of_ball again
                            </li>
                            <li>
If a direction is detected, commander creates a Command message and publishes it on the /command topic
                            </li>
                            <li>
The flight controller receives the command and actuates the quadcopter accordingly
Steps 4-9 are repeated until we kill the ROS node and ROScore with the offboard computer
                            </li>
                        </ol>
                    </div>

                </div>

                <div id="challenges" class="col-md-12">
                    <center><h1>Challenges</h1></center>

                    <p>
We experienced difficulties with both the hardware and software while working on the drone. One issue we ran into was a malfunctioning motor. There were a total of four possible reasons for this malfunction listed as follows: an ESC (electronic speed controller) is burnt out, the pins the ESC is soldered to on flight controller is burnt, the soldered connection between the flight controller and the ESC is not making enough contact, or the motor itself is burnt out. At first we thought the soldered connection were the reason the motor would not spin, so we tested out this theory by first resoldering the connections and added more solder. When this failed we immediately thought the pins we soldered to were burnt out, so we resoldered the ESC to different pins on the board, careful not to desolder one of the other wires or create a bridge and short the flight controller. When this failed to solve the issue, we decided to change out the ESC we were using in hopes that it would solve the problem. We were going to test all the MOSFETs on the ESC to see if one was burnt but we did not have access to a multimeter that had continuity. Again, our assumption that the ESC was the cause for our motor malfunction was incorrect and we realized the motor itself was burnt out. Another hardware issue that we experienced but were unable to resolve was with the transmitter. We had trouble setting up the arming channel correctly on the transmitter and in the rosflight program. This was due to spare documentation and limited forums online that discussed these issues in regards to specifically with rosflight. Since this was not resolved we were unable to fly the drone, and make the motors actuate the commands the flight controller received.
                    </p>
                </div>

                <div id="results" class="col-md-12">
                    <center><h1>Results</h1></center>

                    <p>
Our final result was an almost-working quadcopter that was just unable to fly. Our computer vision and ROS software was able to properly function and send commands to the flight controller. On the hardware side, our drone fully functioned and was controllable with the transmitter when using Betaflight instead of ROSflight. The connection between the Raspberry Pi and flight controller sent the appropriate messages, and the flight controller received them as well. Check out the video!
                    </p>

                    <center>
                        <iframe id="demo_sensing_and_planning" width="560" height="315" src="https://www.youtube.com/embed/94f4LI98am8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </center>

                    <center>
                        <iframe id="demo_rc_transmitter" width="560" height="315" src="https://www.youtube.com/embed/fCW8nTfN-dA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </center>

                    <center>
                        <iframe id="demo_actuation" width="560" height="315" src="https://www.youtube.com/embed/bH-If9QGS5g" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </center>

                    <center>
                        <iframe id="demo_sensing_and_planning2" width="560" height="315" src="https://www.youtube.com/embed/94f4LI98am8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </center>

                </div>

                <div id="conclusion" class="col-md-12">
                    <center><h1>Conclusion</h1></center>

                    <p>
In the end, Acroquad met all of the design requirements except the final flying phase. All of the hardware and software functioned perfectly. Of course, no robotics project is complete without hacks. With the hardware, we attached the webcam to the quadcopter frame with some duct tape--it is, in robotics, a panacea. On the software side, the main hack was with how we called the computer vision code with the ROS node. Because they had been written separately, the computer vision code was originally run as a Python program and not a function. We simply jammed all of the code into a function, get_trace_of_ball, and just called that ugly long function with the ROS node. The final hack was how our offboard computer ran code on the Raspberry Pi. We never got around to creating a script that would start everything necessary for the quadcopter to fly, so instead, we would run the roscore in the background before calling commander. With more time to clean up the hacks, we would definitely design a better 3D-printed mount for the webcam, clean up the code with more articulated functions, and write a shell script that would cleanly take care of running all the software. Furthermore, we can extend our project by improving our computer vision capabilities to be less sensitive to noise and to detect a wider range of movements, including not just a tennis ball, but also human body movements. And it can never hurt to get more powerful hardware, most notably a better (and more expensive) onboard computer.
                    </p>
                    <p>
The one problem that prevented the drone from actually flying was with the ROSflight firmware. ROSflight is set up so that the motors are unable to actuate because of the intense safety features implemented in the firmware. The lack of a properly set arming channel on the transmitter prevented us from being able to actuate the motors via the ROSflight commands or the transmitter. Due to the lack of documentation and resources on how to set up an arming channel for specifically ROSflight, we did not have time to overcome this challenge before our deadline. Basically, everything worked, but ROSflight implemented a safety feature that we could not disarm to get the drone to respond to the commands. In the future we hope to solve this issue and possibly even write documentation for ROSflight on how to overcome this problem.
                    </p>

                </div>

                <div id="additional_information" class="col-md-12">
                    <center><h1>Additional Information</h1></center>

                    Our code is available at our <a href="https://github.com/victorchan314/ee106a_final_project_code">Github</a>.
                </div>

            </div>

        </div>

    </body>

</html>
