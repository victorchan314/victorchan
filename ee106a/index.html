<!DOCTYPE html>
<html lang="en">

    <head>

        <title>EE 106A Final Project</title>

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Latest compiled and minified CSS -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

        <!-- Optional theme -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">

        <!-- Latest compiled and minified JavaScript -->
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

        <link rel="stylesheet" type="text/css" href="styles.css">

    </head>


    <body>

        <div class="container-fluid">

            <div class="main row">

                <img class="img-responsive" src="img/jerome1_cropped.jpg"></img>

                <center><h1>Acroquad</h1></center>

                <nav class="navbar navbar-default">
                    <ul class="nav navbar-nav">
                        <li class="active"><a href="#">Home</a></li>
                        <li><a href="#introduction">Introduction</a></li>
                        <li><a href="#team">Team</a></li>
                        <li><a href="#Design">Design</a></li>
                        <li><a href="#hardware">Hardware</a></li>
                        <li><a href="#software">Software</a></li>
                        <li><a href="#system">System</a></li>
                        <li><a href="#challenges">Challenges</a></li>
                        <li><a href="#results">Results</a></li>
                        <li><a href="#additional_information">Additional Information</a></li>
                    </ul>
                </nav>

                <div id="introduction" class="col-md-12">
                    <center><h1>Introduction</h1></center>

                    <p>
Our Goal: To assemble a UAV with an onboard computer that responds to visual commands by actuating in a corresponding movement.
                    </p>
                    <p>
Unmanned aerial vehicles are currently a popular branch of robotics, with leading researchers integrating computer vision and machine learning to reduce human error in controlling drones. Because both fields are relatively new, current researchers are still developing better and more sophisticated ways to control quadcopters with machine learning! Our group was interested in combining racing drones and computer vision to create a quadcopter that responds to camera inputs. The project was challenging, as we had to not only construct a quadcopter with an onboard computer, but also run an intensive computer vision algorithm on the quadcopter and figure out how to control the drone with the outputs from the algorithm. The applications of such a product are limitless, including autonomous disaster search and rescue, subject tracking in cinematography, and automatic obstacle avoidance for UAVs.
                    </p>

                </div>

                <div id="team" class="col-md-12">
                    <center><h1>Team</h1></center>

                    <table id="team_information" class="table table-responsive">
                        <tr>
                            <td>Name</td>
                            <td>Contact</td>
                            <td>Bio</td>
                            <td>Contribution</td>
                        </tr>
                        <tr>
                            <td>Chong Zhao</td>
                            <td></td>
                            <td>
                            </td>
                            <td>
                            </td>
                        </tr>
                        <tr>
                            <td>Hongjian Duan</td>
                            <td></td>
                            <td>
                            </td>
                            <td>
                            </td>
                        </tr>
                        <tr>
                            <td>Isabella Maceda</td>
                            <td></td>
                            <td>
                            </td>
                            <td>
                            </td>
                        </tr>
                        <tr>
                            <td>Victor Chan</td>
                            <td>victorchan@berkeley.edu</td>
                            <td>
Hi! I am an EECS undergraduate student at UC Berkeley. I have a background in machine learning and robotics, so for me, Acroquad was the perfect combination of esoteric hardware debugging and annoying coding problems. In my spare time I teach the Parkour Decal at Berkeley (check us out) and go bouldering at Bridges!
                            </td>
                            <td>
I worked on everything related to the Raspberry Pi and ROS, from setting up Ubuntu with ROS on the Pi to writing the ROS node to translate between the computer vision outputs and ROSflight commands. I also worked on getting the camera feed running and configuring headless setup of the Pi.
                            </td>
                        </tr>
                    </table>

                </div>

                <div id="design" class="col-md-12">
                    <center><h1>Design</h1></center>

                    <p>
Our final product will be a drone that is controllable without the use of a transmitter. We are aiming to accomplish a task the drone community has started to address in the latest commercial drones, such as the DJI Spark. The applications of such a product are limitless, including disaster search and rescue, subject tracking in cinematography, and automatic obstacle avoidance.
                    </p>
                    <p>
The final product consists of three main components: the Raspberry Pi running ROS, the computer vision model, and the actual quadcopter itself, along with its controls. We will use a small racing quadcopter as the star of the show, running a firmware called ROSflight to translate ROS commands into controls. The quadcopter will be equipped with an onboard Raspberry Pi to process the camera input with computer vision. The model itself, described in more detail in Tasks, may vary as we actually implement the image analysis, but our main ideas include a CNN for edge detection.
                    </p>
                    <p>
Because our project has three modular components, we can augment each of them separately, especially the computer vision model and the commands. We may attempt to use visual inputs other than color, such as hand gestures or shapes, to control the drone. On the other hand, we can also implement more complex controls, including flight planning or path finding. This can be used to track a subject, such as a color rectangle or a face.
                    </p>

                </div>

                <div id="implementation" class="col-md-12">

                    <div id="hardware" class="col-md-12">
                        <center><h1>Hardware</h1></center>

                        <p>
The drone has a Naze 32 F1 flight controller because of its compatibility with the ROSflight firmware. Since we used an F1 controller we had to use a FrSky XRS receiver that had a PPM signal, as specified in the ROSflight documentation. We decided on a Raspberry Pi 3B+ with a 32GB SD card as our onboard computer, with a Logitech webcam for the image processing. Lastly, we used a Taranis QX7 transmitter because it is compatible with our receiver and has an easy-to-use user interface for setting up flight models.
                        </p>
                        <p>
Our quadcopter frame was a generic racing FPV drone frame. It took care of most of the parts, but because we used a webcam instead of an FPV camera, we had to create a custom 3D-print piece to attach the webcam onto the drone. Because it was difficult to design the perfect custom part, we also included a little bit of duct tape to secure the webcam position. The webcam is connected to the Raspberry Pi at a USB port. Finally, we firmly and cleanly fastened the Raspberry Pi to the quadcopter with some double-sided tape and connected it to the F1 controller with a USB-to-microUSB cable.
                        </p>

                    </div>

                    <div id="software" class="col-md-12">
                        <center><h1>Software</h1></center>

                        <p>
Our software consists of two main parts: the ROS nodes, and the computer vision algorithm. Everything is written in Python2.7 and runs on the Raspberry Pi, to which we downloaded Ubuntu 16.04 and installed ROS kinetic. We actually discovered that Ubiquity Robotics provides a Ubuntu 16.04 image with ROS kinetic already installed, which made the flashing process a lot easier.
                        </p>
                        <p>
We have one main ROS node that acts as the connection between the computer vision and the quadcopter. Our ROS package, sight_to_flight, has one ROS node: commander. The ROS node calls the get_trace_of_ball function in the trace_ball package and receives 1 of 4 directions: 0 (up), 1 (down), 2 (left), and 3 (right). commander then generates a new Command message, defined in the ROSflight package, sets the parameters to correspond to the appropriate direction, and publishes it to the /command topic. It then restarts the get_trace_of_ball function and loops until the ROS program is quit.
                        </p>
                        <p>
Our computer vision code is all contained within the trace_ball package. The relevant code is contained within get_trace_of_ball_function.py, which uses OpenCV, NumPy, and Deque. The program begins by setting up parameters such as the max time to run, creates a deque, and opens up the webcam feed. It then runs a loop until it either is killed by commander or times out after about 30 seconds. Within the loop, it keeps track of each frame, at a rate of about 30 FPS, and detects a yellow circle (our image of a tennis ball) with OpenCV’s edge and color detection algorithms. The program compares the current center of the ball to the previously calculated center and determines the most probable direction that the ball has been travelling in (basically just the direction with the largest change) before adding that direction to the deque. After the deque is full, the program determines which direction the tennis ball has travelled in the most, and returns that as the output. If no direction is returned, the program returns -1.
                        </p>

                    </div>

                    <div id="system" class="col-md-12">
                        <center><h1>System</h1></center>

                        <p>
Our system is simply composed of the three separate modules: the quadcopter and its firmware and flight controller, the Raspberry Pi and ROS, and the computer vision and webcam. As described before, the F1 flight controller is connected to the Raspberry Pi with a USB-to-microUSB cable and controlled by ROSflight, and the computer vision outputs are converted to ROSflight commands through the custom ROS node commander. The system runs as follows:
                        </p>

                        <ol>
                            <li>
The quadcopter is turned on, powering the Raspberry Pi as well.
                            </li>
                            <li>
With an offboard computer, we connect to the Pi, which is set up for headless
                            </li>
                            <li>
We run the ROS node commander from the offboard computer
                            </li>
                            <li>
commander calls the computer vision algorithm, get_trace_of_ball
                            </li>
                            <li>
get_trace_of_ball takes in feed from the webcam and returns a direction (or no direction)
                            </li>
                            <li>
If no direction is detected, commander sleeps for 5 seconds and calls get_trace_of_ball again
                            </li>
                            <li>
If a direction is detected, commander creates a Command message and publishes it on the /command topic
                            </li>
                            <li>
The flight controller receives the command and actuates the quadcopter accordingly
Steps 4-9 are repeated until we kill the ROS node and ROScore with the offboard computer
                            </li>
                        </ol>
                    </div>

                </div>

                <div id="challenges" class="col-md-12">
                    <center><h1>Challenges</h1></center>

                    <p>
We experienced difficulties with both the hardware and software while working on the drone. One issue we ran into was a malfunctioning motor. There were a total of four possible reasons for this malfunction listed as follows: an ESC (electronic speed controller) is burnt out, the pins the ESC is soldered to on flight controller is burnt, the soldered connection between the flight controller and the ESC is not making enough contact, or the motor itself is burnt out. At first we thought the soldered connection were the reason the motor would not spin, so we tested out this theory by first resoldering the connections and added more solder. When this failed we immediately thought the pins we soldered to were burnt out, so we resoldered the ESC to different pins on the board, careful not to desolder one of the other wires or create a bridge and short the flight controller. When this failed to solve the issue, we decided to change out the ESC we were using in hopes that it would solve the problem. We were going to test all the MOSFETs on the ESC to see if one was burnt but we did not have access to a multimeter that had continuity. Again, our assumption that the ESC was the cause for our motor malfunction was incorrect and we realized the motor itself was burnt out. Another hardware issue that we experienced but were unable to resolve was with the transmitter. We had trouble setting up the arming channel correctly on the transmitter and in the rosflight program. This was due to spare documentation and limited forums online that discussed these issues in regards to specifically with rosflight. Since this was not resolved we were unable to fly the drone, and make the motors actuate the commands the flight controller received.
                    </p>
                </div>

                <div id="results" class="col-md-12">
                    <center><h1>Results</h1></center>

                    <p>
Our final result was we had functioning computer vision and ROS software that was able to send movement commands to flight controller and we had a fully functioning drone. The reason the motors were unable to actuate with the rosflight firmware was due to the intense safety features implemented in the firmware the lack of a properly set arming channel on the transmitter prevented use from being able to actuate the motors via code or the transmitter. Due to the lack of documentation and resources on how to set up an arming channel for specifically rosflight, we did not have the time to overcome this challenge before our deadline. In the future we hope to solve this issue and improve our computer vision capabilities and get access to a more powerful onboard computer.
                    </p>
                </div>

                <div id="additional_information" class="col-md-12">
                    <center><h1>Additional Information</h1></center>
                </div>

            </div>

        </div>

    </body>

</html>
